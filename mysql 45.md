### mysql 45讲

> #### 第 1 讲

1. server层，包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 
   * 连接器，负责跟客户端建立连接、获取权限、维持和管理连接。 （长连接问题）
   * 查询缓存 ，不推荐使用。
   * 分析器，分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句， MySQL需要识别出里面的字符串分别是什么，代表什么；做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。 
   * 优化器，经过了分析器， MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。
     优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时
     候，决定各个表的连接顺序。 
   * 执行器，MySQL通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误； 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。  
   
   ```
   在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟rows_examined并不是完全相同的。
   ```
   
2. 存储引擎，负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、 MyISAM、 Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。 







> #### 第 2 讲

1. redo log（重做日志）
   * 当有一条记录需要更新的时候， InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时， InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。
   *  InnoDB的redo log是固定大小的 。
   * 有了redo log， InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe 。
   * redo log是InnoDB引擎特有的日志 。
   
2. binlog（归档日志）
   
   * binlog是MySQL的Server层实现的，所有引擎都可以使用（binlog没有crash-safe能力）。
   
3. redo log和binlog区别

   * redo log是InnoDB引擎特有的； binlog是MySQL的Server层实现的，所有引擎都可以使用。
   * redo log是物理日志，记录的是“在某个数据页上做了什么修改”； binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 
   * redo log是循环写的，空间固定会用完； binlog是可以追加写入的。 “追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 
   
4. update时候的内部流程（update table1 set value = value + 1 where id = 2）

   * 执行器先找引擎取ID=2这一行。 ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。
     
* 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。
     
   * 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。
  
   * 执行器生成这个操作的binlog，并把binlog写入磁盘。
   
* 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。 
   
```
   redo log的写入被拆成了两个步骤： prepare和commit，这就是"两阶段提交"。
   mysql还有回滚日志（undo log）,错误日志（error log）,慢查询日志（slow query log）等等。
```
   
5. [关于日志的更详细的解释](https://www.cnblogs.com/wy123/p/8365234.html)







> #### 第 3 讲

1. 事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在MySQL中，事务支持是在引擎层实现的。MySQL是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如MySQL原生的MyISAM引擎就不支持事务（这也是MyISAM被InnoDB取代的重要原因之一）。 

2. ACID，原子性，一致性，隔离性，持久性。

3. 隔离性：为了解决数据库上有多个事务同时执行的时候可能出现脏读（dirty read）、不可重复读（nonrepeatable read）、幻读（phantom read）的问题，就有了隔离级别的概念。SQL标准的事务隔离级别包括：<b>读未提交</b>（read uncommitted）、<b>读提交</b>（read committed）、<b>可重复读</b>（repeatable read）和<b>串行化</b>（serializable ）。 

   * 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到。
   * 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到。
   * 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。
     当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。
   * 串行化，顾名思义是对于同一行记录， “写”会加“写锁”， “读”会加“读锁”。当出现读写锁冲突的时
     候，后访问的事务必须等前一个事务执行完成，才能继续执行。

   ```
   在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是， “读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。
   Oracle数据库的默认隔离级别其实就是“读提交”。
   ```


4. 事务隔离的实现（回滚日志，长事务，可以在information_schema库的innodb_trx这个表中查询长事务）

5. 事务的启动方式








> #### 第 4 讲

1. 索引，在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工
   作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。 
2. 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的， InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。每一个索引在InnoDB里面对应一棵B+树。 
3. 根据叶子节点的内容，索引类型分为主键索引和非主键索引。
   主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。
   非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。 
4. 基于主键索引和普通索引的查询的区别：主键查询方式，则只需要搜索主键这棵B+树；普通索引查询方式，则需要先搜索k索引树，得到主键的值，再到主键索引树搜索一次。这个过程称为<b>回表</b>。也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。  

5. 自增主键，指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。 也就是说，自增主键的插入数据模式，每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。 而且当主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 因此从性能和存储空间方面考量， 自增主键往往是更合理的选择。 在只有一个索引而且该索引是唯一索引的情况下才考虑使用业务字段直接做主键。







> #### 第 5 讲

1. 覆盖索引，例如：

   ```sql
   create table T (
   id int primary key,
   k int NOT NULL DEFAULT 0,
   s varchar(16) NOT NULL DEFAULT '',
   index k(k)) engine=InnoDB;
   insert into T values(100,1, 'aa'),(200,2,'bb'),(300,3,'cc'),(500,5,'ee'),(600,6,'f'),(700,7,'gg');
   select id from T where k between 3 and 5
   ```

   这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。

2. 联合索引 & 最左前缀

   * 在建立联合索引的时候， 第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 
   * 最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符 。
   * 因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了。 
   * 如果既有联合查询，又有基于a、 b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候你不得不维护另外一个索引，也就是说你需要同时维护(a,b)、 (b) 这两个索引。 此时要考虑的原则就是空间了。 

3. 索引下推

   * MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。（针对联合索引来说） 

```txt
在满足语句需求的情况下， 尽量少地访问资源是数据库设计的重要原则之一。
```







> #### 第 6 讲

1. 锁

   * 全局锁：全局锁就是对整个数据库实例加锁。 MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。全局锁的典型使用场景是，做全库逻辑备份。（如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态）。

     官方自带的逻辑备份工具是mysqldump。在可重复读隔离级别下，当mysqldump使用参数single-transaction的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的（前提是引擎要支持这个隔离级别）。

   * 表级锁

     * 表锁，lock tables … read/write。 与FTWRL类似，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。需要注意， lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。 

     * 元数据锁MDL，MDL不需要显式使用，在访问一个表的时候会被自动加上。 MDL的作用是，保证读写的正确性。事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。  

       在MySQL 5.5版本中引入了MDL，<b>当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。</b>
       <b>读锁之间不互斥</b>，因此你可以有多个线程同时对一张表增删改查。
       <b>读写锁之间、写锁之间是互斥的</b>，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 

     ```
     如何安全地给小表加字段？
     1. 首先我们要解决长事务，事务不提交，就会一直占着MDL锁。在MySQL的information_schema 库的innodb_trx 表中，你可以查到当前执行中的事务。如果你要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。
     ```
  2. 如果是热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程。
     
     






> #### 第 7 讲

1. MySQL的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。 InnoDB是支持行锁的，这也是MyISAM被InnoDB替代的重要原因之一。 

2. 两阶段锁

   * 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时（即commit时）才释放。这个就是两阶段锁协议。因此，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。  
   
3. 死锁 & 死锁检测

   * 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 

   * 当出现死锁以后，有两种策略：
     一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。（mysql 默认超时时间为50s不可接受）
     另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 （死锁检测会耗费大量的cpu资源）

   * 解决：

     * 如果能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉（有风险）
     * 控制并发度（需要使用中间件）

     * 可以考虑通过将一行改成逻辑上的多行来减少锁冲突







> #### 第 8 讲

1. InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id。
2. 在实现上， InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。 “活跃”指的就是，启动了但还没提交。数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。
   而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。（麻烦）
3. 查询的逻辑：一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况（简洁）
   * 版本未提交，不可见；
   * 版本已提交，但是是在视图创建后提交的，不可见；
   * 版本已提交，而且是在视图创建前提交的，可见 。
4. 更新的逻辑：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。 
5. 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前
   的记录的行锁被其他事务占用的话，就需要进入锁等待。 







> #### 第 9 讲

1. 唯一索引 & 普通索引

   * 查询的时候，对于普通索引来说，查找到满足条件的第一个记录后，需要查找下一个记录，直到碰到第一
     个不满足条件的记录；对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索 。两者的差距微乎其微。

   * 更新的时候， 对于<b>唯一索引</b>来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入(4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这<b>必须要将数据页读入内存才能判断</b>，因此，<b>唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用</b>。 
     * 这个记录要更新的目标页在内存中 ：两者无差别
     * 这个记录要更新的目标页不在内存中 ：对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；对于普通索引来说，则是将更新记录在change buffer，语句执行就结束了。 

   ```
   change buffer : 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下， InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 需要说明的是，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说， change buffer在内存中有拷贝，也会被写入到磁盘上。将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭（shutdown）的过程中，也会执行merge操作。
   
   显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。
   
   change buffer用的是buffer pool里的内存，因此不能无限增大。 change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%。
   
   将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。 change buffer因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。
   
   使用场景，因为merge的时候是真正进行数据更新的时刻，而change buffer的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做merge之前， change buffer记录的变更越多（也就是这个页面上要更新的次数多），收益就越大。因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价。所以，对于这种业务模式来说， change buffer反而起到了副作用。
   ```

   总结：普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，建议尽量选择普通索引。如果所有的更新后面，都马上伴随着对这个记录的查询，那么就应该关闭change buffer。而在其他情况下， change buffer都能提升更新性能。 